{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1 13scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import os \n",
    "%matplotlib inline\n",
    "np.random.seed(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# import keras\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Convolution2D, MaxPool2D, BatchNormalization, SeparableConv2D, Activation\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint\n",
    "from keras.layers import Input\n",
    "from tensorflow.python.keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.models import load_model\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "\n",
    "# import PIL\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "# import scipy\n",
    "from scipy.signal import convolve2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 13\n",
    "input_size = 256\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define VGG19 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 8, 8, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               8388864   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 13)                3341      \n",
      "=================================================================\n",
      "Total params: 28,417,613\n",
      "Trainable params: 28,417,101\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG19\n",
    "def vgg19_model():\n",
    "    model = Sequential()\n",
    "    model.add(VGG19(include_top=False, weights='imagenet', classes=num_class, input_shape=(256,256,3)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_class, activation='softmax'))\n",
    "    #model.layers[0].trainable = False\n",
    "    return model\n",
    "    pass\n",
    "classifier = vgg19_model()\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "input_1                        False\n",
      "----------------------------------------\n",
      "block1_conv1                   False\n",
      "----------------------------------------\n",
      "block1_conv2                   False\n",
      "----------------------------------------\n",
      "block1_pool                    False\n",
      "----------------------------------------\n",
      "block2_conv1                   False\n",
      "----------------------------------------\n",
      "block2_conv2                   False\n",
      "----------------------------------------\n",
      "block2_pool                    False\n",
      "----------------------------------------\n",
      "block3_conv1                   False\n",
      "----------------------------------------\n",
      "block3_conv2                   False\n",
      "----------------------------------------\n",
      "block3_conv3                   False\n",
      "----------------------------------------\n",
      "block3_conv4                   False\n",
      "----------------------------------------\n",
      "block3_pool                    False\n",
      "----------------------------------------\n",
      "block4_conv1                   False\n",
      "----------------------------------------\n",
      "block4_conv2                   False\n",
      "----------------------------------------\n",
      "block4_conv3                   False\n",
      "----------------------------------------\n",
      "block4_conv4                   False\n",
      "----------------------------------------\n",
      "block4_pool                    False\n",
      "----------------------------------------\n",
      "block5_conv1                   False\n",
      "----------------------------------------\n",
      "block5_conv2                   False\n",
      "----------------------------------------\n",
      "block5_conv3                   False\n",
      "----------------------------------------\n",
      "block5_conv4                   False\n",
      "----------------------------------------\n",
      "block5_pool                    False\n",
      "----------------------------------------\n",
      "flatten_1                       True\n",
      "----------------------------------------\n",
      "dense_1                         True\n",
      "----------------------------------------\n",
      "batch_normalization_1           True\n",
      "----------------------------------------\n",
      "activation_1                    True\n",
      "----------------------------------------\n",
      "dropout_1                       True\n",
      "----------------------------------------\n",
      "dense_2                         True\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(classifier.layers):\n",
    "    if i == 0:\n",
    "        for layer0 in classifier.layers[i].layers[:]:\n",
    "            layer0.trainable = False\n",
    "            print('-'*40)\n",
    "            print('%-25s'%layer0.name, '%10s'%layer0.trainable)\n",
    "    else:\n",
    "        print('-'*40)\n",
    "        print('%-25s'%layer.name, '%10s'%layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import 13-scene Dataset & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_size(full_data=False, validation_split_ratio=0.2):\n",
    "    if full_data:\n",
    "        valid_split = 0.05\n",
    "    else:\n",
    "        valid_split = validation_split_ratio\n",
    "        \n",
    "    train_datagen = ImageDataGenerator(\n",
    "        height_shift_range=0.2,\n",
    "        width_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        validation_split=valid_split)\n",
    "\n",
    "    valid_datagen = ImageDataGenerator(\n",
    "        validation_split=valid_split)\n",
    "    \n",
    "    return train_datagen, valid_datagen\n",
    "train_datagen, valid_datagen = training_size(full_data=True, validation_split_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3672 images belonging to 13 classes.\n",
      "Found 187 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(\n",
    "    'scene/data',\n",
    "    target_size=(256, 256),\n",
    "    subset='training',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    seed=3023)\n",
    "\n",
    "validation_set = valid_datagen.flow_from_directory(\n",
    "    'scene/data',\n",
    "    target_size=(256, 256),\n",
    "    subset='validation',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=3023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "# optimizer = SGD(lr=0.0001, momentum=0.9)\n",
    "\n",
    "#classifier = vgg19_model()\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save model when meets the highest validation set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = ModelCheckpoint('pretrain_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "58/58 [==============================] - 90s 2s/step - loss: 0.7266 - accuracy: 0.7590 - val_loss: 0.7863 - val_accuracy: 0.8663\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.86631, saving model to 13SCENE_bs64_spe58.h5\n",
      "Epoch 2/30\n",
      "58/58 [==============================] - 75s 1s/step - loss: 0.3271 - accuracy: 0.8903 - val_loss: 0.1846 - val_accuracy: 0.9198\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.86631 to 0.91979, saving model to 13SCENE_bs64_spe58.h5\n",
      "Epoch 3/30\n",
      "58/58 [==============================] - 77s 1s/step - loss: 0.2525 - accuracy: 0.9156 - val_loss: 0.2355 - val_accuracy: 0.9412\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.91979 to 0.94118, saving model to 13SCENE_bs64_spe58.h5\n",
      "Epoch 4/30\n",
      "58/58 [==============================] - 76s 1s/step - loss: 0.2251 - accuracy: 0.9243 - val_loss: 0.2506 - val_accuracy: 0.9144\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.94118\n",
      "Epoch 5/30\n",
      "58/58 [==============================] - 77s 1s/step - loss: 0.1958 - accuracy: 0.9368 - val_loss: 0.2768 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.94118\n",
      "Epoch 6/30\n",
      "58/58 [==============================] - 77s 1s/step - loss: 0.1680 - accuracy: 0.9491 - val_loss: 0.2167 - val_accuracy: 0.9144\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.94118\n",
      "Epoch 7/30\n",
      "58/58 [==============================] - 77s 1s/step - loss: 0.1653 - accuracy: 0.9453 - val_loss: 0.1206 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.94118\n",
      "Epoch 8/30\n",
      "58/58 [==============================] - 76s 1s/step - loss: 0.1509 - accuracy: 0.9504 - val_loss: 0.2367 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.94118\n",
      "Epoch 9/30\n",
      "58/58 [==============================] - 77s 1s/step - loss: 0.1336 - accuracy: 0.9572 - val_loss: 0.1337 - val_accuracy: 0.9465\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.94118 to 0.94652, saving model to 13SCENE_bs64_spe58.h5\n",
      "Epoch 10/30\n",
      "58/58 [==============================] - 78s 1s/step - loss: 0.1124 - accuracy: 0.9627 - val_loss: 0.1111 - val_accuracy: 0.9358\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.94652\n",
      "Epoch 11/30\n",
      "58/58 [==============================] - 78s 1s/step - loss: 0.1192 - accuracy: 0.9627 - val_loss: 0.1147 - val_accuracy: 0.9358\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.94652\n",
      "Epoch 12/30\n",
      "58/58 [==============================] - 78s 1s/step - loss: 0.1109 - accuracy: 0.9676 - val_loss: 0.1647 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.94652\n",
      "Epoch 13/30\n",
      "58/58 [==============================] - 79s 1s/step - loss: 0.0992 - accuracy: 0.9670 - val_loss: 0.1632 - val_accuracy: 0.9198\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.94652\n",
      "Epoch 14/30\n",
      "58/58 [==============================] - 79s 1s/step - loss: 0.1050 - accuracy: 0.9676 - val_loss: 0.1117 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.94652\n",
      "Epoch 15/30\n",
      "58/58 [==============================] - 79s 1s/step - loss: 0.1058 - accuracy: 0.9643 - val_loss: 0.1650 - val_accuracy: 0.9198\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.94652\n",
      "Epoch 16/30\n",
      "58/58 [==============================] - 79s 1s/step - loss: 0.0867 - accuracy: 0.9722 - val_loss: 0.1435 - val_accuracy: 0.9358\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.94652\n",
      "Epoch 17/30\n",
      "58/58 [==============================] - 79s 1s/step - loss: 0.0901 - accuracy: 0.9700 - val_loss: 0.2297 - val_accuracy: 0.9091\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.94652\n",
      "Epoch 18/30\n",
      "58/58 [==============================] - 79s 1s/step - loss: 0.0786 - accuracy: 0.9749 - val_loss: 0.1167 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.94652\n",
      "Epoch 19/30\n",
      "58/58 [==============================] - 80s 1s/step - loss: 0.0886 - accuracy: 0.9733 - val_loss: 0.1399 - val_accuracy: 0.8984\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.94652\n",
      "Epoch 20/30\n",
      "58/58 [==============================] - 80s 1s/step - loss: 0.0795 - accuracy: 0.9749 - val_loss: 0.1087 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.94652\n",
      "Epoch 21/30\n",
      "58/58 [==============================] - 79s 1s/step - loss: 0.0773 - accuracy: 0.9747 - val_loss: 0.1086 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.94652\n",
      "Epoch 22/30\n",
      "58/58 [==============================] - 79s 1s/step - loss: 0.0702 - accuracy: 0.9752 - val_loss: 0.1454 - val_accuracy: 0.9037\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.94652\n",
      "Epoch 23/30\n",
      "58/58 [==============================] - 79s 1s/step - loss: 0.0878 - accuracy: 0.9690 - val_loss: 0.1120 - val_accuracy: 0.9198\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.94652\n",
      "Epoch 24/30\n",
      "58/58 [==============================] - 79s 1s/step - loss: 0.0724 - accuracy: 0.9798 - val_loss: 0.1002 - val_accuracy: 0.9144\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.94652\n",
      "Epoch 25/30\n",
      "58/58 [==============================] - 80s 1s/step - loss: 0.0760 - accuracy: 0.9749 - val_loss: 0.1543 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.94652\n",
      "Epoch 26/30\n",
      "58/58 [==============================] - 79s 1s/step - loss: 0.0608 - accuracy: 0.9807 - val_loss: 0.1504 - val_accuracy: 0.9358\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.94652\n",
      "Epoch 27/30\n",
      "58/58 [==============================] - 79s 1s/step - loss: 0.0660 - accuracy: 0.9804 - val_loss: 0.1433 - val_accuracy: 0.9305\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.94652\n",
      "Epoch 28/30\n",
      "58/58 [==============================] - 79s 1s/step - loss: 0.0584 - accuracy: 0.9815 - val_loss: 0.1187 - val_accuracy: 0.9251\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.94652\n",
      "Epoch 29/30\n",
      "58/58 [==============================] - 79s 1s/step - loss: 0.0675 - accuracy: 0.9769 - val_loss: 0.1027 - val_accuracy: 0.9198\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.94652\n",
      "Epoch 30/30\n",
      "58/58 [==============================] - 79s 1s/step - loss: 0.0592 - accuracy: 0.9837 - val_loss: 0.0605 - val_accuracy: 0.9358\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.94652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc870226f98>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(\n",
    "    training_set,\n",
    "    steps_per_epoch=58,\n",
    "    epochs=30,\n",
    "    validation_data=validation_set,\n",
    "    validation_steps=32.5,\n",
    "    verbose=1,\n",
    "    callbacks = [ac]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
